# AuraFLooow âœ¨  
_Image Generation Without the VRAM Meltdown_

[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

So you've got a mid-range GPU, an artistic itch, and a mild aversion to ComfyUIâ€™s 400-node spaghetti graphs? Welcome. Youâ€™re in the right place â€” **AuraFLooow** is your one-click, low-fuss web UI for running the AuraFlow image model without setting your rig on fire.

This isn't a feature dump of ComfyUI, and it doesnâ€™t want to be. Think of it as â€œimage generation on a dietâ€ â€” built for people who just want to type stuff like _â€œcyberpunk goose in a trench coatâ€_ and see it rendered, without reading a TensorRT whitepaper first.

---

## ğŸ§ What *Is* This?

AuraFLooow is a lightweight web app using Gradio that wraps around the excellent [`fal/AuraFlow-v0.3`](https://huggingface.co/fal/AuraFlow-v0.3) text-to-image model. Itâ€™s for those who:

- Own a GPU that didnâ€™t cost more than their car.
- Prefer sliders over shell scripts.
- Want to run locally (yes, even offline â€” no "cloud credits" required).
- Think AI art should be simple enough that your mum could use it. And possibly does.

---

## ğŸ¯ Core Features (aka â€œWhat It Actually Doesâ€)

- Generates images from text prompts with AuraFlow.
- Resize image output â€” up to AuraFlowâ€™s very generous limits.
- Tweak steps, CFG scale, and seed (or roll the dice).
- Saves your creations to an `output/` folder automatically.
- Dark-mode-ish modern UI with buttons, not bash.
- Tries very hard not to gobble up all your VRAM, with CPU offloading via `diffusers`.

---

## ğŸ§  System Requirements

### ğŸ§± Hardware
- A **dedicated NVIDIA GPU** with **at least 6GB of VRAM**.
  - *Can it run on CPU?* Yes. Should it? Not unless you enjoy existential waiting.
  - *AMD GPUs?* Technically possible. But we donâ€™t officially support those shenanigans â€” `diffusers` + ROCm is a whole separate odyssey.

### ğŸ’¿ Software
- **Python 3.9+**  
- **Pip** (comes with Python, usually)
- **Git** (optional, unless you enjoy downloading ZIPs manually)

### ğŸ§™ Required Python Packages

Everything you need is in `requirements.txt`, including:
- `diffusers`, `torch`, `transformers`, `gradio`, `accelerate`
- Plus a few supporting roles: `Pillow`, `protobuf`, `safetensors`, etc.

---

## ğŸ› ï¸ Installation â€“ Or â€œHow Not to Break Itâ€

### 1. Clone (or Download) the Repository
**Option A â€“ Git enjoyers:**
```bash
git clone https://github.com/Raxephion/AuraFlooow.git AuraFLooow
cd AuraFLooow
```

**Option B â€“ ZIP warriors:**
Download the ZIP from GitHub, extract it, and `cd` into the folder.

### 2. Create a Virtual Environment
Because isolation is healthy (for Python projects, anyway):
```bash
python -m venv venv
```

### 3. Activate It
**Windows:**
```bash
venv\Scripts\activate
```
**macOS/Linux:**
```bash
source venv/bin/activate
```

### 4. Install Dependencies
```bash
pip install -r requirements.txt
```
Yes, PyTorch is big. Go make coffee.

---

## ğŸš€ Running the App

After installing, fire it up:

```bash
python app.py
```

(Windows users can just double-click `run.bat` if the command line makes them itchy.)

The **first run** will download the model (10â€“15GB, depending on version). Donâ€™t panic â€” itâ€™s only slow once. After that, itâ€™s cached.

Once you see something like:
```
Running on local URL: http://127.0.0.1:7860
```
pop that into your browser and enjoy the ride.

---

## ğŸ§© About the Model â€“ AuraFlow v0.3

AuraFlow v0.3 is a clever little beast â€” flow-based, open-source, and fine-tuned for aesthetics. Think of it as the model equivalent of a gallery curator with a GPU.

> _â€œSupports multiple aspect ratios, 1536px limits, and state-of-the-art scores on GenEval.â€_  
> â€“ paraphrased from [fal.ai](https://fal.ai/blog/auraflow)

Massive kudos to the team: `@cloneofsimo`, `@isidentical`, and others who actually understand diffusion math.

---

## ğŸ§¯ Common Problems (and Solutions That Donâ€™t Involve Screaming)

### ğŸ”¥ CUDA Out of Memory?
- Lower your image resolution.
- Close other GPU-intensive apps (yes, even Chrome).
- 6GB VRAM is the sweet spot â€” below that, expect hiccups.
- CPU offload helps, but it's not witchcraft.

### ğŸ¢ First Run is Slow?
Thatâ€™s the model downloading. Itâ€™s not stuck. Just slow. Blame your ISP.

### âŒ Protobuf Errors or Missing Models?
Double-check that youâ€™re installing packages *inside* the virtual environment.

### ğŸ§ª Gradio `tooltip` errors?
We removed `tooltip` for broader compatibility. If youâ€™re using a newer Gradio version and really want it back â€” go ahead and hack it in.

---

## ğŸ” Changing the Model

Default model is `fal/AuraFlow-v0.3`. To swap it:

1. Open `app.py`
2. Look for `MODEL_NAME = "fal/AuraFlow-v0.3"`
3. Change it to your preferred model (as long as itâ€™s AuraFlow-compatible)

You can also load models locally if theyâ€™re in your Hugging Face cache.

---

## ğŸ™ƒ Final Words (and a Gentle Disclaimer)

This is a side project. Itâ€™s not bulletproof. Itâ€™s not enterprise-ready. But it works, and it's meant to get you creating without the overhead of ComfyUI or the GPU of a data center.

Please:
- Use ethically.
- Donâ€™t prompt weird illegal stuff.
- Credit the original model creators.
- Donâ€™t send support requests to NVIDIA when this breaks.

---

Happy prompting! ğŸ¨  
â€” *Team Probably Didnâ€™t Sleep Enough*
